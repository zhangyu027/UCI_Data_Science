{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a74039",
   "metadata": {},
   "source": [
    " LSTMs as a highly reliable model that exploits the patterns in stock data perfectly, or can be used blindly without any human-in-the-loop. I did this as an experiment, in a pure machine learning interest. In my opinion, the model has observed certain patterns in the data, thus giving it the ability to correctly predict the stock movements most of the time. But it is up to you to decide if this model can be used for practical purposes or not.\n",
    "\n",
    "Why Do You Need Time Series Models?\n",
    "You would like to model stock prices correctly, so as a stock buyer you can reasonably decide when to buy stocks and when to sell them to make a profit. This is where time series modeling comes in. You need good machine learning models that can look at the history of a sequence of data and correctly predict what the future elements of the sequence are going to be.\n",
    "\n",
    "Warning: Stock market prices are highly unpredictable and volatile. This means that there are no consistent patterns in the data that allow you to model stock prices over time near-perfectly. Don’t take it from me, take it from Princeton University economist Burton Malkiel, who argues in his 1973 book, “A Random Walk Down Wall Street,” that if the market is truly efficient and a share price reflects all factors immediately as soon as they’re made public, a blindfolded monkey throwing darts at a newspaper stock listing should do as well as any investment professional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694cc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make sure that you have all these libaries available to run the code successfully\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json \n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf # This code has been tested with TensorFlow 1.6\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953564fa",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42057107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame by date\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Double check the result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70d5fe9",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0312fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(df.shape[0]),(df['Low']+df['High'])/2.0)\n",
    "plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Mid Price',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b5ff3",
   "metadata": {},
   "source": [
    "## Splitting Data into a Training set and a Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2c85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First calculate the mid prices from the highest and lowest\n",
    "high_prices = df.loc[:,'High'].as_matrix()\n",
    "low_prices = df.loc[:,'Low'].as_matrix()\n",
    "mid_prices = (high_prices+low_prices)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = mid_prices[:11000] \n",
    "test_data = mid_prices[11000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e5924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data to be between 0 and 1\n",
    "# When scaling remember! You normalize both test and train data with respect to training data\n",
    "# Because you are not supposed to have access to test data\n",
    "scaler = MinMaxScaler()\n",
    "train_data = train_data.reshape(-1,1)\n",
    "test_data = test_data.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c199ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Scaler with training data and smooth data\n",
    "smoothing_window_size = 2500\n",
    "for di in range(0,10000,smoothing_window_size):\n",
    "    scaler.fit(train_data[di:di+smoothing_window_size,:])\n",
    "    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n",
    "\n",
    "# You normalize the last bit of remaining data\n",
    "scaler.fit(train_data[di+smoothing_window_size:,:])\n",
    "train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afded84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape the data back to the shape of [data_size]\n",
    "\n",
    "\n",
    "# Reshape both train and test data\n",
    "train_data = train_data.reshape(-1)\n",
    "\n",
    "# Normalize test data\n",
    "test_data = scaler.transform(test_data).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now perform exponential moving average smoothing\n",
    "# So the data will have a smoother curve than the original ragged data\n",
    "EMA = 0.0\n",
    "gamma = 0.1\n",
    "for ti in range(11000):\n",
    "  EMA = gamma*train_data[ti] + (1-gamma)*EMA\n",
    "  train_data[ti] = EMA\n",
    "\n",
    "# Used for visualization and test purposes\n",
    "all_mid_data = np.concatenate([train_data,test_data],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf68a1a",
   "metadata": {},
   "source": [
    "## Evaluating Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93233dd0",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = 1 # Dimensionality of the data. Since your data is 1-D this would be 1\n",
    "num_unrollings = 50 # Number of time steps you look into the future.\n",
    "batch_size = 500 # Number of samples in a batch\n",
    "num_nodes = [200,200,150] # Number of hidden nodes in each layer of the deep LSTM stack we're using\n",
    "n_layers = len(num_nodes) # number of layers\n",
    "dropout = 0.2 # dropout amount\n",
    "\n",
    "tf.reset_default_graph() # This is important in case you run this multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbeae96",
   "metadata": {},
   "source": [
    "## Defining Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input data.\n",
    "train_inputs, train_outputs = [],[]\n",
    "\n",
    "# You unroll the input over time defining placeholders for each time step\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,D],name='train_inputs_%d'%ui))\n",
    "    train_outputs.append(tf.placeholder(tf.float32, shape=[batch_size,1], name = 'train_outputs_%d'%ui))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec67277",
   "metadata": {},
   "source": [
    "## Defining Parameters of the LSTM and Regression layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7266fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_cells = [\n",
    "    tf.contrib.rnn.LSTMCell(num_units=num_nodes[li],\n",
    "                            state_is_tuple=True,\n",
    "                            initializer= tf.contrib.layers.xavier_initializer()\n",
    "                           )\n",
    " for li in range(n_layers)]\n",
    "\n",
    "drop_lstm_cells = [tf.contrib.rnn.DropoutWrapper(\n",
    "    lstm, input_keep_prob=1.0,output_keep_prob=1.0-dropout, state_keep_prob=1.0-dropout\n",
    ") for lstm in lstm_cells]\n",
    "drop_multi_cell = tf.contrib.rnn.MultiRNNCell(drop_lstm_cells)\n",
    "multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n",
    "\n",
    "w = tf.get_variable('w',shape=[num_nodes[-1], 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable('b',initializer=tf.random_uniform([1],-0.1,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc4514",
   "metadata": {},
   "source": [
    "## Calculating LSTM output and Feeding it to the regression layer to get final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create cell state and hidden state variables to maintain the state of the LSTM\n",
    "c, h = [],[]\n",
    "initial_state = []\n",
    "for li in range(n_layers):\n",
    "  c.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "  h.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "  initial_state.append(tf.contrib.rnn.LSTMStateTuple(c[li], h[li]))\n",
    "\n",
    "# Do several tensor transofmations, because the function dynamic_rnn requires the output to be of\n",
    "# a specific format. Read more at: https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "all_inputs = tf.concat([tf.expand_dims(t,0) for t in train_inputs],axis=0)\n",
    "\n",
    "# all_outputs is [seq_length, batch_size, num_nodes]\n",
    "all_lstm_outputs, state = tf.nn.dynamic_rnn(\n",
    "    drop_multi_cell, all_inputs, initial_state=tuple(initial_state),\n",
    "    time_major = True, dtype=tf.float32)\n",
    "\n",
    "all_lstm_outputs = tf.reshape(all_lstm_outputs, [batch_size*num_unrollings,num_nodes[-1]])\n",
    "\n",
    "all_outputs = tf.nn.xw_plus_b(all_lstm_outputs,w,b)\n",
    "\n",
    "split_outputs = tf.split(all_outputs,num_unrollings,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b619d0e",
   "metadata": {},
   "source": [
    "## Loss Calculation and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a505b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "When calculating the loss you need to be careful about the exact form, because you calculate\n",
    "# loss of all the unrolled steps at the same time\n",
    "# Therefore, take the mean error or each batch and get the sum of that over all the unrolled steps\n",
    "\n",
    "print('Defining training Loss')\n",
    "loss = 0.0\n",
    "with tf.control_dependencies([tf.assign(c[li], state[li][0]) for li in range(n_layers)]+\n",
    "                             [tf.assign(h[li], state[li][1]) for li in range(n_layers)]):\n",
    "  for ui in range(num_unrollings):\n",
    "    loss += tf.reduce_mean(0.5*(split_outputs[ui]-train_outputs[ui])**2)\n",
    "\n",
    "print('Learning rate decay operations')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "tf_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "tf_min_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "\n",
    "learning_rate = tf.maximum(\n",
    "    tf.train.exponential_decay(tf_learning_rate, global_step, decay_steps=1, decay_rate=0.5, staircase=True),\n",
    "    tf_min_learning_rate)\n",
    "\n",
    "# Optimizer.\n",
    "print('TF Optimization operations')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))\n",
    "\n",
    "print('\\tAll done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb26f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Defining prediction related TF functions')\n",
    "\n",
    "sample_inputs = tf.placeholder(tf.float32, shape=[1,D])\n",
    "\n",
    "# Maintaining LSTM state for prediction stage\n",
    "sample_c, sample_h, initial_sample_state = [],[],[]\n",
    "for li in range(n_layers):\n",
    "  sample_c.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "  sample_h.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "  initial_sample_state.append(tf.contrib.rnn.LSTMStateTuple(sample_c[li],sample_h[li]))\n",
    "\n",
    "reset_sample_states = tf.group(*[tf.assign(sample_c[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)],\n",
    "                               *[tf.assign(sample_h[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)])\n",
    "\n",
    "sample_outputs, sample_state = tf.nn.dynamic_rnn(multi_cell, tf.expand_dims(sample_inputs,0),\n",
    "                                   initial_state=tuple(initial_sample_state),\n",
    "                                   time_major = True,\n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "with tf.control_dependencies([tf.assign(sample_c[li],sample_state[li][0]) for li in range(n_layers)]+\n",
    "                              [tf.assign(sample_h[li],sample_state[li][1]) for li in range(n_layers)]):  \n",
    "  sample_prediction = tf.nn.xw_plus_b(tf.reshape(sample_outputs,[1,-1]), w, b)\n",
    "\n",
    "print('\\tAll done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba497b",
   "metadata": {},
   "source": [
    "## Running the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f76a4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "I’m hoping that you found this tutorial useful. I should mention that this was a rewarding experience for me. In this tutorial, I learnt how difficult it can be to device a model that is able to correctly predict stock price movements. You started with a motivation for why you need to model stock prices. This was followed by an explanation and code for downloading data. Then you looked at two averaging techniques that allow you to make predictions one step into the future. You next saw that these methods are futile when you need to predict more than one step into the future. Thereafter you discussed how you can use LSTMs to make predictions many steps into the future. Finally you visualized the results and saw that your model (though not perfect) is quite good at correctly predicting stock price movements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
